{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: E-GMD Dataset Exploration & Subsetting Strategy\n",
    "\n",
    "**Objective:** Explore the structure and metadata of the Expanded Groove MIDI Dataset (E-GMD) to understand its characteristics and devise a strategy for creating a manageable, representative subset for model development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure plots\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Dataset Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the relative path to the root directory where the E-GMD dataset was extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path relative to the notebooks directory\n",
    "DATASET_PATH: Path = Path(\"../data/raw/e-gmd-v1.0.0/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the path exists\n",
    "if not DATASET_PATH.exists():\n",
    "    print(f\"Error: Dataset path not found at {DATASET_PATH.resolve()}\")\n",
    "else:\n",
    "    print(f\"Dataset path found: {DATASET_PATH.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the main metadata file (`e-gmd-v1.0.0.csv`) into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA_FILE: Path = DATASET_PATH / \"e-gmd-v1.0.0.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not METADATA_FILE.exists():\n",
    "    print(f\"Error: Metadata file not found at {METADATA_FILE.resolve()}\")\n",
    "else:\n",
    "    print(f\"Loading metadata from: {METADATA_FILE.resolve()}\")\n",
    "    try:\n",
    "        metadata_df: pd.DataFrame = pd.read_csv(METADATA_FILE)\n",
    "        print(\"Metadata loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading metadata: {e}\")\n",
    "        metadata_df = pd.DataFrame() # Assign empty df on error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform initial exploration of the metadata to understand the available fields, data types, and distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Basic Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not metadata_df.empty:\n",
    "    print(\"First 5 rows:\")\n",
    "    display(metadata_df.head())\n",
    "    print(\"\\nDataFrame Info:\")\n",
    "    metadata_df.info()\n",
    "else:\n",
    "    print(\"Metadata DataFrame is empty, skipping exploration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Descriptive Statistics (Numerical Columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not metadata_df.empty:\n",
    "    print(\"Descriptive Statistics:\")\n",
    "    display(metadata_df.describe())\n",
    "else:\n",
    "    print(\"Metadata DataFrame is empty, skipping statistics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Value Counts (Categorical Columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the distribution of key categorical features like `drummer`, `session`, `style`, and `time_signature`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not metadata_df.empty:\n",
    "    categorical_cols: typing.List[str] = ['drummer', 'session', 'style', 'time_signature', 'split']\n",
    "    for col in categorical_cols:\n",
    "        if col in metadata_df.columns:\n",
    "            print(f\"\\n--- Value Counts for '{col}' ---\")\n",
    "            print(metadata_df[col].value_counts())\n",
    "        else:\n",
    "            print(f\"\\nColumn '{col}' not found in metadata.\")\n",
    "else:\n",
    "    print(\"Metadata DataFrame is empty, skipping value counts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Distribution Visualization (Numerical Columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the distribution of numerical columns like `tempo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not metadata_df.empty:\n",
    "    numerical_cols: typing.List[str] = ['bpm', 'duration'] # Add other relevant numerical cols if needed\n",
    "    for col in numerical_cols:\n",
    "        if col in metadata_df.columns:\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            sns.histplot(metadata_df[col], kde=True)\n",
    "            plt.title(f\"Distribution of {col}\")\n",
    "            plt.xlabel(col.capitalize())\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"\\nColumn '{col}' not found for visualization.\")\n",
    "else:\n",
    "    print(\"Metadata DataFrame is empty, skipping visualizations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Propose Stratification Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the metadata exploration above, particularly the value counts for categorical columns, propose a strategy for stratified sampling to create a balanced 10% subset of the data. Consider which column(s) would be most suitable for stratification to ensure the subset represents the diversity of the full dataset (e.g., across different drummers, styles, tempos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposed Strategy:**\n",
    "\n",
    "*   *(Analyze the output of the cells above, especially section 4.3, to fill this in)*\n",
    "*   **Primary Stratification Column:** [Propose column, e.g., `style` or `drummer`]\n",
    "*   **Reasoning:** [Explain why this column is suitable, e.g., captures musical diversity, has reasonable category counts]\n",
    "*   **Secondary Stratification (Optional):** [Consider if another column like `tempo` (binned) or `time_signature` should be used for finer stratification]\n",
    "*   **Target Subset Size:** ~10% of the total dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for User:** Does this proposed stratification strategy seem appropriate? Should we proceed with implementing the sampling based on [Proposed Column(s)]?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Implement Subset Extraction Pipeline\n",
    "\n",
    "Now, we implement the stratified sampling based on the `drummer` column to create a 10% subset. We will then copy the corresponding audio and MIDI files to a new directory and save the subset's metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure metadata_df is loaded and not empty before proceeding\n",
    "if 'metadata_df' in locals() and not metadata_df.empty:\n",
    "    print(\"Performing stratified sampling based on 'drummer'...\")\n",
    "    # Group by drummer and sample 10% from each group\n",
    "    df_subset = metadata_df.groupby('drummer', group_keys=False).sample(frac=0.1, random_state=42)\n",
    "\n",
    "    print(\"\\nSubset created. First 5 rows:\")\n",
    "    display(df_subset.head())\n",
    "\n",
    "    print(f\"\\nShape of the subset DataFrame: {df_subset.shape}\")\n",
    "else:\n",
    "    print(\"Metadata DataFrame not loaded or empty. Skipping subset creation.\")\n",
    "    df_subset = pd.DataFrame() # Assign empty df if metadata wasn't loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify stratification by comparing drummer distributions\n",
    "if not df_subset.empty:\n",
    "    print(\"Comparing drummer distribution (Original vs Subset):\\n\")\n",
    "\n",
    "    original_counts = metadata_df['drummer'].value_counts(normalize=True).sort_index()\n",
    "    subset_counts = df_subset['drummer'].value_counts(normalize=True).sort_index()\n",
    "\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Original (%)': original_counts * 100,\n",
    "        'Subset (%)': subset_counts * 100\n",
    "    })\n",
    "    comparison_df['Difference (%)'] = comparison_df['Subset (%)'] - comparison_df['Original (%)']\n",
    "\n",
    "    display(comparison_df.style.format('{:.2f}%'))\n",
    "\n",
    "    # Check if proportions are roughly similar (allowing for sampling variance)\n",
    "    if comparison_df['Difference (%)'].abs().max() < 5: # Allow up to 5% difference\n",
    "         print(\"\\nStratification appears successful (proportions are similar).\")\n",
    "    else:\n",
    "         print(\"\\nWarning: Stratification proportions differ significantly. Check sampling.\")\n",
    "\n",
    "else:\n",
    "    print(\"Subset DataFrame is empty. Skipping stratification verification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Create Subset Directory and Copy Files\n",
    "\n",
    "Define the path for the subset data and create the necessary directory structure. Then, iterate through the subset metadata, copying the corresponding audio (`.wav`) and MIDI (`.mid`) files from the original dataset path to the new subset path, maintaining the `split/drummer/session/style` structure. We use `shutil.copy2` to preserve metadata and `tqdm` for progress tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import typing # Ensure typing is imported if not already\n",
    "\n",
    "# Define subset path relative to the project root (notebooks directory's parent)\n",
    "# Note: Adjust if your project structure differs\n",
    "SUBSET_PATH: Path = Path('../data/subset/')\n",
    "DATASET_PATH: Path = Path(\"../data/raw/e-gmd-v1.0.0/\") # Ensure this is correctly defined earlier\n",
    "\n",
    "if not df_subset.empty:\n",
    "    print(f\"Creating subset directory at: {SUBSET_PATH.resolve()}\")\n",
    "    os.makedirs(SUBSET_PATH, exist_ok=True)\n",
    "\n",
    "    print(\"Copying files to subset directory...\")\n",
    "    copied_files_count = 0\n",
    "    skipped_files_count = 0\n",
    "\n",
    "    # Use tqdm for progress bar\n",
    "    for index, row in tqdm(df_subset.iterrows(), total=df_subset.shape[0], desc=\"Copying files\"):\n",
    "        # Construct source paths (assuming structure: DATASET_PATH / split / audio_filename)\n",
    "        # audio_filename often includes drummer/session/style/etc.\n",
    "        source_audio_path: Path = DATASET_PATH / row['split'] / row['audio_filename']\n",
    "        source_midi_path: Path = DATASET_PATH / row['split'] / row['midi_filename']\n",
    "\n",
    "        # Construct target paths, maintaining structure within SUBSET_PATH\n",
    "        target_audio_path: Path = SUBSET_PATH / row['split'] / row['audio_filename']\n",
    "        target_midi_path: Path = SUBSET_PATH / row['split'] / row['midi_filename']\n",
    "\n",
    "        # Create necessary subdirectories in the target path\n",
    "        target_audio_dir: Path = target_audio_path.parent\n",
    "        target_midi_dir: Path = target_midi_path.parent\n",
    "        os.makedirs(target_audio_dir, exist_ok=True)\n",
    "        # No need to create midi dir separately if it's the same as audio dir,\n",
    "        # but doesn't hurt if they might differ in some edge case.\n",
    "        os.makedirs(target_midi_dir, exist_ok=True)\n",
    "\n",
    "        # Copy files if they exist, using copy2 to preserve metadata\n",
    "        try:\n",
    "            if source_audio_path.exists():\n",
    "                shutil.copy2(source_audio_path, target_audio_path)\n",
    "                copied_files_count += 1\n",
    "            else:\n",
    "                print(f\"Warning: Source audio file not found: {source_audio_path}\")\n",
    "                skipped_files_count += 1\n",
    "\n",
    "            if source_midi_path.exists():\n",
    "                shutil.copy2(source_midi_path, target_midi_path)\n",
    "                # Assuming 1 audio = 1 midi, only increment copied_files_count once per pair\n",
    "            else:\n",
    "                print(f\"Warning: Source MIDI file not found: {source_midi_path}\")\n",
    "                # If audio was copied but MIDI wasn't, it's still a partial success\n",
    "                # Adjust counting logic if needed\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error copying files for index {index}: {e}\")\n",
    "            skipped_files_count += 1 # Count as skipped if error occurs\n",
    "\n",
    "    print(f\"\\nFile copying complete.\")\n",
    "    print(f\"Successfully copied pairs (audio+midi): {copied_files_count}\")\n",
    "    print(f\"Skipped files/pairs due to errors or missing source: {skipped_files_count}\")\n",
    "\n",
    "else:\n",
    "    print(\"Subset DataFrame is empty. Skipping file copying.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Save Subset Metadata\n",
    "\n",
    "Finally, save the `df_subset` DataFrame containing the metadata for the newly created subset to a CSV file within the subset directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_subset.empty:\n",
    "    subset_metadata_path: Path = SUBSET_PATH / 'subset_metadata.csv'\n",
    "    print(f\"Saving subset metadata to: {subset_metadata_path.resolve()}\")\n",
    "    try:\n",
    "        df_subset.to_csv(subset_metadata_path, index=False)\n",
    "        print(\"Subset metadata saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving subset metadata: {e}\")\n",
    "else:\n",
    "    print(\"Subset DataFrame is empty. Skipping metadata saving.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Train/Validation/Test Splits\n",
    "\n",
    "Now that we have the subset metadata, we'll split it into training (70%), validation (15%), and test (15%) sets. We will stratify the split based on the `drummer` column to ensure each drummer is proportionally represented in all sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1. Load Subset Metadata (if needed)\n",
    "\n",
    "First, let's load the subset metadata file we created in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import typing\n",
    "\n",
    "# Define path to subset metadata\n",
    "SUBSET_METADATA_PATH: Path = Path('../data/subset/subset_metadata.csv')\n",
    "\n",
    "# Load the dataframe\n",
    "if SUBSET_METADATA_PATH.exists():\n",
    "    print(f\"Loading subset metadata from: {SUBSET_METADATA_PATH.resolve()}\")\n",
    "    try:\n",
    "        df_subset: pd.DataFrame = pd.read_csv(SUBSET_METADATA_PATH)\n",
    "        print(\"Subset metadata loaded successfully.\")\n",
    "        display(df_subset.head())\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading subset metadata: {e}\")\n",
    "        df_subset = pd.DataFrame() # Assign empty df on error\n",
    "else:\n",
    "    print(f\"Error: Subset metadata file not found at {SUBSET_METADATA_PATH.resolve()}\")\n",
    "    df_subset = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. Perform Stratified Split\n",
    "\n",
    "We'll use `sklearn.model_selection.train_test_split` twice to achieve the 70/15/15 split, stratified by `drummer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_STATE: int = 42\n",
    "TRAIN_SIZE: float = 0.70 # 70% for training\n",
    "VAL_TEST_SIZE: float = 0.30 # Remaining 30% for validation + test\n",
    "VAL_SIZE_REL_TEMP: float = 0.50 # 50% of the temp set for validation (0.30 * 0.50 = 0.15 of total)\n",
    "TEST_SIZE_REL_TEMP: float = 0.50 # 50% of the temp set for test (0.30 * 0.50 = 0.15 of total)\n",
    "\n",
    "if not df_subset.empty:\n",
    "    print(\"Performing train/validation/test split...\")\n",
    "\n",
    "    # Ensure the index is unique if it's not already (needed for mapping back)\n",
    "    if not df_subset.index.is_unique:\n",
    "        df_subset = df_subset.reset_index(drop=True)\n",
    "\n",
    "    # Features (X) and target/stratification column (y)\n",
    "    X = df_subset.index # Use index to easily map back\n",
    "    y = df_subset['drummer']\n",
    "\n",
    "    # First split: Train (70%) and Temp (30%)\n",
    "    X_train_idx, X_temp_idx, y_train, y_temp = train_test_split(\n",
    "        X, y,\n",
    "        train_size=TRAIN_SIZE,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=y,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Second split: Validation (15%) and Test (15%) from Temp (30%)\n",
    "    # Need to stratify based on the drummer labels corresponding to the temp indices\n",
    "    y_temp_stratify = df_subset.loc[X_temp_idx, 'drummer']\n",
    "    X_val_idx, X_test_idx, y_val, y_test = train_test_split(\n",
    "        X_temp_idx, y_temp_stratify,\n",
    "        train_size=VAL_SIZE_REL_TEMP, # Relative to the temp set size\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=y_temp_stratify,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    print(\"Splitting complete.\")\n",
    "    print(f\"Train set size: {len(X_train_idx)}\")\n",
    "    print(f\"Validation set size: {len(X_val_idx)}\")\n",
    "    print(f\"Test set size: {len(X_test_idx)}\")\n",
    "\n",
    "else:\n",
    "    print(\"Subset DataFrame is empty. Skipping split.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3. Add 'split' Column to DataFrame\n",
    "\n",
    "Now, add a column to the original `df_subset` indicating which split each row belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_subset.empty and 'X_train_idx' in locals():\n",
    "    # Add the 'split' column\n",
    "    df_subset['split_set'] = 'unknown' # Initialize column\n",
    "    df_subset.loc[X_train_idx, 'split_set'] = 'train'\n",
    "    df_subset.loc[X_val_idx, 'split_set'] = 'validation'\n",
    "    df_subset.loc[X_test_idx, 'split_set'] = 'test'\n",
    "\n",
    "    print(\"Added 'split_set' column to the DataFrame.\")\n",
    "    display(df_subset.head())\n",
    "else:\n",
    "    print(\"Split indices not available or DataFrame empty. Skipping adding 'split_set' column.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4. Verify Split Proportions and Stratification\n",
    "\n",
    "Let's check the counts for each split and the distribution of drummers within each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "if not df_subset.empty and 'split_set' in df_subset.columns:\n",
    "    print(\"--- Split Set Value Counts ---\")\n",
    "    split_counts = df_subset['split_set'].value_counts(normalize=True) * 100\n",
    "    print(split_counts)\n",
    "\n",
    "    print(\"\\n--- Drummer Distribution per Split Set ---\")\n",
    "    # Calculate drummer distribution within each split\n",
    "    drummer_dist = df_subset.groupby('split_set')['drummer'].value_counts(normalize=True).unstack(level=0) * 100\n",
    "\n",
    "    # Display the distribution table\n",
    "    display(drummer_dist.style.format('{:.2f}%').background_gradient(cmap='viridis', axis=1))\n",
    "\n",
    "    # Optional: Visualize the distribution\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    sns.countplot(data=df_subset, y='drummer', hue='split_set', order=df_subset['drummer'].value_counts().index)\n",
    "    plt.title('Drummer Distribution Across Train/Validation/Test Splits')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Drummer')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Check if proportions are roughly correct\n",
    "    target_proportions = {'train': 70.0, 'validation': 15.0, 'test': 15.0}\n",
    "    all_proportions_ok = True\n",
    "    for split_name, target_prop in target_proportions.items():\n",
    "        actual_prop = split_counts.get(split_name, 0)\n",
    "        if not np.isclose(actual_prop, target_prop, atol=2.0): # Allow 2% tolerance\n",
    "             print(f\"Warning: Proportion for '{split_name}' ({actual_prop:.2f}%) is off target ({target_prop:.1f}%).\")\n",
    "             all_proportions_ok = False\n",
    "\n",
    "    if all_proportions_ok:\n",
    "        print(\"\\nSplit proportions are within tolerance.\")\n",
    "\n",
    "    # Check stratification (visual inspection or more rigorous statistical tests could be added)\n",
    "    print(\"\\nVisually inspect the table and plot above to confirm drummer distribution is similar across splits.\")\n",
    "\n",
    "else:\n",
    "    print(\"Subset DataFrame is empty or 'split_set' column not found. Skipping verification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5. Save Updated Subset Metadata\n",
    "\n",
    "Finally, save the `df_subset` DataFrame, now including the `split_set` column, back to the CSV file, overwriting the previous version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_subset.empty and 'split_set' in df_subset.columns:\n",
    "    print(f\"Saving updated subset metadata to: {SUBSET_METADATA_PATH.resolve()}\")\n",
    "    try:\n",
    "        # Ensure the original 'split' column (from E-GMD) isn't confused with our new 'split_set'\n",
    "        # If the original 'split' column exists and is not needed, consider dropping it before saving\n",
    "        # df_to_save = df_subset.drop(columns=['split'], errors='ignore') # Example if needed\n",
    "\n",
    "        df_subset.to_csv(SUBSET_METADATA_PATH, index=False)\n",
    "        print(\"Updated subset metadata saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving updated subset metadata: {e}\")\n",
    "else:\n",
    "    print(\"Subset DataFrame is empty or 'split_set' column missing. Skipping metadata saving.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Subset Examples\n",
    "\n",
    "Let's visualize a couple of random examples from our subset to get a feel for the audio and corresponding MIDI drum patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports for visualization\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pretty_midi\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import typing\n",
    "\n",
    "# Ensure subset dataframe is loaded (it should be from previous steps)\n",
    "SUBSET_METADATA_PATH: Path = Path('../data/subset/subset_metadata.csv')\n",
    "SUBSET_DATA_DIR: Path = Path('../data/subset/')\n",
    "\n",
    "if 'df_subset' not in locals() or df_subset.empty:\n",
    "    if SUBSET_METADATA_PATH.exists():\n",
    "        print(f\"Reloading subset metadata from: {SUBSET_METADATA_PATH.resolve()}\")\n",
    "        try:\n",
    "            df_subset: pd.DataFrame = pd.read_csv(SUBSET_METADATA_PATH)\n",
    "            print(\"Subset metadata reloaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reloading subset metadata: {e}\")\n",
    "            df_subset = pd.DataFrame() # Assign empty df on error\n",
    "    else:\n",
    "        print(f\"Error: Subset metadata file not found at {SUBSET_METADATA_PATH.resolve()}\")\n",
    "        df_subset = pd.DataFrame()\n",
    "\n",
    "# Define a function to plot piano roll for drums\n",
    "def plot_drum_piano_roll(pm: pretty_midi.PrettyMIDI, start_pitch: int = 35, end_pitch: int = 51, fs: int = 100):\n",
    "    \"\"\"\n",
    "    Plots a piano roll representation for drum notes.\n",
    "\n",
    "    Args:\n",
    "        pm (pretty_midi.PrettyMIDI): The PrettyMIDI object.\n",
    "        start_pitch (int): The lowest MIDI pitch to include in the plot.\n",
    "        end_pitch (int): The highest MIDI pitch to include in the plot.\n",
    "        fs (int): Sampling frequency for the piano roll grid.\n",
    "    \"\"\"\n",
    "    # Find the drum instrument\n",
    "    drum_instrument = None\n",
    "    for instrument in pm.instruments:\n",
    "        if instrument.is_drum:\n",
    "            drum_instrument = instrument\n",
    "            break\n",
    "    \n",
    "    if drum_instrument is None:\n",
    "        print(\"No drum track found in MIDI.\")\n",
    "        return\n",
    "\n",
    "    # Get piano roll\n",
    "    piano_roll = drum_instrument.get_piano_roll(fs=fs)\n",
    "    # Keep only relevant pitches\n",
    "    piano_roll = piano_roll[start_pitch:end_pitch + 1, :]\n",
    "\n",
    "    # Plot\n",
    "    librosa.display.specshow(piano_roll, \n",
    "                             hop_length=1, \n",
    "                             sr=fs, \n",
    "                             x_axis='time', \n",
    "                             y_axis='cqt_note', \n",
    "                             fmin=pretty_midi.note_number_to_hz(start_pitch),\n",
    "                             cmap='magma')\n",
    "    plt.yticks(np.arange(start_pitch, end_pitch + 1), \n",
    "               [pretty_midi.note_number_to_drum_name(n) for n in range(start_pitch, end_pitch + 1)])\n",
    "    plt.ylabel(\"Drum Kit\")\n",
    "    plt.title('Drum Piano Roll')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_subset.empty:\n",
    "    # Select 2 random examples\n",
    "    num_examples_to_show = 2\n",
    "    random_indices = random.sample(range(len(df_subset)), num_examples_to_show)\n",
    "    example_rows = df_subset.iloc[random_indices]\n",
    "\n",
    "    for index, row in example_rows.iterrows():\n",
    "        print(f\"\\n--- Visualizing Example {index} ---\")\n",
    "        # Use 'bpm' column for tempo display\n",
    "        print(f\"Drummer: {row['drummer']}, Style: {row['style']}, Tempo: {row['bpm']:.2f} BPM, Split: {row['split_set']}\")\n",
    "\n",
    "        # Construct file paths\n",
    "        # Path structure: SUBSET_DATA_DIR / split / audio_filename (which includes drummer/session/etc)\n",
    "        audio_path: Path = SUBSET_DATA_DIR / row['split'] / row['audio_filename']\n",
    "        midi_path: Path = SUBSET_DATA_DIR / row['split'] / row['midi_filename']\n",
    "\n",
    "        print(f\"Audio Path: {audio_path}\")\n",
    "        print(f\"MIDI Path: {midi_path}\")\n",
    "\n",
    "        try:\n",
    "            # Load audio\n",
    "            y, sr = librosa.load(audio_path, sr=None) # Load with original sample rate\n",
    "            print(f\"Audio loaded: duration={librosa.get_duration(y=y, sr=sr):.2f}s, sample_rate={sr}Hz\")\n",
    "\n",
    "            # Load MIDI\n",
    "            pm = pretty_midi.PrettyMIDI(str(midi_path))\n",
    "            print(f\"MIDI loaded: estimated tempo={pm.estimate_tempo():.2f} BPM\")\n",
    "\n",
    "            # Create plots\n",
    "            plt.figure(figsize=(15, 8))\n",
    "\n",
    "            # Plot 1: Waveform\n",
    "            plt.subplot(2, 1, 1)\n",
    "            librosa.display.waveshow(y, sr=sr)\n",
    "            plt.title(f\"Audio Waveform - {row['audio_filename']}\")\n",
    "            plt.xlabel(\"Time (s)\")\n",
    "            plt.ylabel(\"Amplitude\")\n",
    "\n",
    "            # Plot 2: Drum Piano Roll\n",
    "            plt.subplot(2, 1, 2)\n",
    "            plot_drum_piano_roll(pm, fs=sr) # Use audio sample rate for fs in piano roll for alignment\n",
    "            # Adjust x-axis limits to match waveform\n",
    "            plt.xlim([0, librosa.get_duration(y=y, sr=sr)]) \n",
    "            plt.xlabel(\"Time (s)\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Audio or MIDI file not found for index {index}. Skipping visualization.\")\n",
    "            print(f\"  Expected Audio: {audio_path}\")\n",
    "            print(f\"  Expected MIDI: {midi_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred processing index {index}: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Subset DataFrame is empty. Cannot visualize examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Subset Statistics\n",
    "\n",
    "Finally, let's calculate and display some key statistics about our created subset to provide a final overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_subset.empty:\n",
    "    print(\"--- Subset Statistics ---\")\n",
    "\n",
    "    # Total number of files\n",
    "    total_files = len(df_subset)\n",
    "    print(f\"Total number of files (audio/MIDI pairs): {total_files}\")\n",
    "\n",
    "    # Total duration\n",
    "    if 'duration' in df_subset.columns:\n",
    "        total_duration_seconds = df_subset['duration'].sum()\n",
    "        total_duration_hours = total_duration_seconds / 3600\n",
    "        print(f\"Total duration of audio: {total_duration_seconds:.2f} seconds (~{total_duration_hours:.2f} hours)\")\n",
    "    else:\n",
    "        print(\"Warning: 'duration' column not found in metadata. Cannot calculate total duration.\")\n",
    "\n",
    "else:\n",
    "    print(\"Subset DataFrame is empty. Cannot generate statistics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1. Subset Distributions\n",
    "\n",
    "Re-displaying the distributions for key features specifically for the subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "if not df_subset.empty:\n",
    "    # Columns to visualize\n",
    "    categorical_cols_subset: typing.List[str] = ['drummer', 'style', 'time_signature', 'split_set']\n",
    "    # Use 'bpm' column for tempo visualization\n",
    "    numerical_cols_subset: typing.List[str] = ['bpm', 'duration']\n",
    "\n",
    "    print(\"\\n--- Subset Categorical Distributions ---\")\n",
    "    for col in categorical_cols_subset:\n",
    "        if col in df_subset.columns:\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            sns.countplot(data=df_subset, y=col, order=df_subset[col].value_counts().index, palette='viridis')\n",
    "            plt.title(f\"Distribution of '{col}' in Subset\")\n",
    "            plt.xlabel(\"Count\")\n",
    "            plt.ylabel(col.capitalize())\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Column '{col}' not found in subset metadata.\")\n",
    "\n",
    "    print(\"\\n--- Subset Numerical Distributions ---\")\n",
    "    for col in numerical_cols_subset:\n",
    "        if col in df_subset.columns:\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            sns.histplot(df_subset[col], kde=True)\n",
    "            plt.title(f\"Distribution of '{col}' in Subset\")\n",
    "            plt.xlabel(col.capitalize())\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Column '{col}' not found for visualization.\")\n",
    "\n",
    "else:\n",
    "    print(\"Subset DataFrame is empty, skipping distribution visualizations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End of Notebook 1.**\n",
    "\n",
    "We have successfully explored the E-GMD dataset, created a stratified 10% subset, copied the relevant files, split the subset into train/validation/test sets, visualized some examples, and generated final statistics. The subset metadata is saved in `data/subset/subset_metadata.csv`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
